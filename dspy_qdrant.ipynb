{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\"https://www.thoughtworks.com/en-in/insights/blog/data-strategy/building-an-amazon-com-for-your-data-products\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_contents = [doc.text for doc in documents]\n",
    "doc_ids = list(range(1, len(doc_contents) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "client.add(\n",
    "    collection_name=\"DSpy_Qdrant\",\n",
    "    documents=doc_contents,\n",
    "    ids=doc_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.retrieve.qdrant_rm import QdrantRM\n",
    "import dspy\n",
    "\n",
    "\n",
    "qdrant_retriever_model = QdrantRM(\"DSpy_Qdrant\", client, k=10)\n",
    "\n",
    "\n",
    "ollama_model = dspy.OllamaLocal(model=\"mistral\",model_type='text',\n",
    "                                max_tokens=350,\n",
    "                                temperature=0.1,\n",
    "                                top_p=0.8, frequency_penalty=1.17, top_k=40)\n",
    "\n",
    "\n",
    "dspy.settings.configure(lm= ollama_model, rm=qdrant_retriever_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 training examples: [Example({'question': 'What characteristics should data products have according to Zhamak Dehgahi?', 'answer': 'Data products should be discoverable, addressable, trustworthy, self-describing, interoperable, and secure.'}) (input_keys={'question'}), Example({'question': 'Why is it considered a lost opportunity that most data products only support one or two use cases?', 'answer': \"It's a lost opportunity because it reflects underutilization and limits the potential benefits across various teams, especially in organizations with decentralized structures or those implementing data mesh.\"}) (input_keys={'question'}), Example({'question': 'What is the benefit of creating a central marketplace or catalog for internal data products?', 'answer': 'Creating a central marketplace or catalog helps raise awareness and can convince skeptical data consumers to start using the data products.'}) (input_keys={'question'})]\n",
      "First 3 development examples: [Example({'question': 'what are tools used in?', 'answer': 'Snowflake, Talend, DBT, Collibra, Monte Carlo, Dataops.live, SOLE, OAM Client libraries'}) (input_keys={'question'}), Example({'question': \"According to Zhamak Dehghani's principles, effective data products in a Data Mesh architecture should possess several key qualities. Which of the following options correctly lists these qualities?\", 'answer': 'Discoverable, Addressable, Trustworthy, Self-Describing, Interoperable, and Secure'}) (input_keys={'question'}), Example({'question': 'The three pillars of Data Mesh success', 'answer': 'Organizational change, product thinking, and technology'}) (input_keys={'question'})]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dspy\n",
    "\n",
    "df = pd.read_csv(\"sample_example.csv\")\n",
    "\n",
    "full_dataset = []\n",
    "for context, question, answer in df.values:\n",
    "    example = dspy.Example(question=question, answer=answer).with_inputs(\"question\")\n",
    "    full_dataset.append(example)\n",
    "\n",
    "split_index = int(len(full_dataset) * 0.7)\n",
    "\n",
    "\n",
    "trainset = full_dataset[:split_index]\n",
    "devset = full_dataset[split_index:]\n",
    "\n",
    "\n",
    "print(\"First 3 training examples:\", trainset[:3])\n",
    "print(\"First 3 development examples:\", devset[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RAG pipeline\n",
    "\n",
    "### Question Answering Concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'context, question -> answer'"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GenerateAnswer.signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer questions with short factoid answers.'"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GenerateAnswer.instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template(Answer questions with short factoid answers., ['Context:', 'Question:', 'Answer:'])\n"
     ]
    }
   ],
   "source": [
    "from dspy.signatures import signature_to_template\n",
    "template = signature_to_template(GenerateAnswer)\n",
    "\n",
    "print(str(template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the considerations to design the right data product?\n",
      "Answer: Key considerations in designing the right data products are its fulfillment to the use case for a given domain, along with compliance to slo and slis, support for output ports based on persona, metadata for discoverability and access and quality aspects to deliver trust.\n"
     ]
    }
   ],
   "source": [
    "print(template.query(dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_passages=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=prediction.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Answer: Thoughtworks identifies potential data products by collecting and analyzing large amounts of data from various sources like articles, blogs, books, podcasts, etc. They collaborate with clients and stakeholders to understand their needs and challenges better. Through experimentation using different technologies and methodologies, they create prototypes or proofs of concept for potential data products that can be tested in real-world scenarios before being scaled up. Based on feedback from stakeholders and users, they continuously improve their data products through iterative development cycles to ensure they meet the evolving needs of their clients and industry trends.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "qa = RAG()\n",
    "\n",
    "pred = qa(\"How does Thoughtworks identify potential data products?\")\n",
    "\n",
    "print(f\"Predicted Answer: {pred.answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling the RAG program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:40<00:00, 25.12s/it]\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "# Validation logic: check that the predicted answer is correct.\n",
    "# Also check that the retrieved context does actually contain that answer.\n",
    "def validate_context_and_answer(example, pred, trace=None):\n",
    "    answer_EM = dspy.evaluate.answer_exact_match(example, pred)\n",
    "    answer_PM = dspy.evaluate.answer_passage_match(example, pred)\n",
    "    return answer_EM and answer_PM\n",
    "\n",
    "# Set up a basic teleprompter, which will compile our RAG program.\n",
    "teleprompter = BootstrapFewShot(metric=validate_context_and_answer)\n",
    "\n",
    "\n",
    "# Compile!\n",
    "compiled_rag = teleprompter.compile(RAG(), trainset=trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executing the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is Data Mesh\n",
      "Predicted Answer: The term \"Data Mesh\" refers to a decentralized data management system where multiple sources of data are interconnected and work together as a network. In this system, each node or entity contributes its own unique perspective on the problem, and these perspectives are combined to form a more comprehensive understanding.\n",
      "\n",
      "The term \"Data Mesh\" was first introduced by ThoughtWorks in 2019 as an alternative to traditional data management systems that rely on centralized databases and monolithic architecture. In contrast, Data Mesh is designed to be flexible, scalable, and resilient, allowing it to adapt quickly to changing conditions and handle large volumes of data from multiple sources.\n",
      "\n",
      "The key characteristics of a Data Mesh include:\n",
      "\n",
      "1. Decentralized architecture: The system is designed as a network of interconnected nodes or entities that contribute their own unique perspective on the problem.\n",
      "2. Real-time data processing: Data is processed in real-time, allowing for immediate insights and decision-making.\n",
      "3. Scalability: The system can handle large volumes of data from multiple sources without compromising performance.\n",
      "4. Flexibility: The architecture allows for easy integration of new data sources or entities as needed.\n",
      "5. Resilience: The decentralized nature of the system makes it more resilient to failures or outages compared to traditional centralized systems.\n",
      "6. Machine learning and AI capabilities: Data Mesh can leverage machine learning and AI techniques to analyze data from multiple sources, identify patterns, and make predictions or recommendations.\n",
      "7. Real-time analytics: The system provides real-\n",
      "Retrieved Contexts (truncated): ['[ ![Thoughtworks](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/thoughtworks-logo.svg) ](/en-in \"Thoughtworks\")\\n\\nMenu\\n\\nClose\\n\\n  * [What we do  ](/en-in/what-we-do \"What we d...']\n"
     ]
    }
   ],
   "source": [
    "my_question = \"What is Data Mesh\"\n",
    "\n",
    "pred = compiled_rag(my_question)\n",
    "\n",
    "print(f\"Question: {my_question}\")\n",
    "print(f\"Predicted Answer: {pred.answer}\")\n",
    "print(f\"Retrieved Contexts (truncated): {[c[:200] + '...' for c in pred.context]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama_model.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_answer\n",
      "Example({'question': 'The three pillars of Data Mesh success', 'answer': 'Organizational change,product thinking, and technology'}) (input_keys={'question'})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, parameter in compiled_rag.named_predictors():\n",
    "    print(name)\n",
    "    print(parameter.demos[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring and Optimizing Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Metric 1: Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://apps.dtic.mil/sti/tr/pdf/AD0667273.pdf\n",
    "def automated_readability_index(text):\n",
    "    import re\n",
    "    characters = len(re.sub(r'\\s+', '', text)) # Count characters (ignoring whitespace)\n",
    "    words = len(text.split()) # Count words by splitting the text\n",
    "    # Count sentences by finding period, exclamation, or question mark\n",
    "    sentences = len(re.findall(r'[.!?\\n]', text))\n",
    "    # small change is to add a new line character as grug doesn't seem to use punctuation.\n",
    "    if words == 0 or sentences == 0:  # Prevent division by zero\n",
    "        return 0\n",
    "    # Calculate the Automated Readability Index (ARI)\n",
    "    ari = (4.71 * (characters / words)) + (0.5 * (words / sentences)) - 21.43\n",
    "    \n",
    "    return round(ari, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI 14.66 => 25.6\n",
      "ARI 8.77 => 23.64\n",
      "ARI 10.56 => 15.76\n",
      "ARI 9.65 => 18.38\n",
      "ARI 15.04 => 14.71\n",
      "ARI 6.62 => 9.64\n",
      "ARI 8.06 => 24.98\n",
      "ARI -1.03 => 17.04\n",
      "ARI 14.98 => 0\n",
      "ARI 0 => 0\n",
      "ARI -1.77 => 0\n"
     ]
    }
   ],
   "source": [
    "for ex in full_dataset:\n",
    "    source_ari = automated_readability_index(ex.question)\n",
    "    grug_ari = automated_readability_index(ex.answer)\n",
    "    print(f\"ARI {source_ari} => {grug_ari}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://dspy-docs.vercel.app/docs/building-blocks/metrics#intermediate-using-ai-feedback-for-your-metric\n",
    "class AssessBasedOnQuestion(dspy.Signature):\n",
    "    \"\"\"Given the assessed text provide a yes or no to the assessment question.\"\"\"\n",
    "    assessed_text = dspy.InputField(format=str)\n",
    "    assessment_question = dspy.InputField(format=str)\n",
    "    assessment_answer = dspy.OutputField(desc=\"Yes or No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assessed Text: This is a test.\n",
      "Assessment Question: Is this a test?\n",
      "Assessment Answer: Yes\n"
     ]
    }
   ],
   "source": [
    "example_question_assessment = dspy.Example(assessed_text=\"This is a test.\", assessment_question=\"Is this a test?\", assessment_answer=\"Yes\").with_inputs(\"assessed_text\", \"assessment_question\")\n",
    "print(signature_to_template(AssessBasedOnQuestion).query(example_question_assessment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_metric(truth, pred, trace=None):\n",
    "    question = truth.question\n",
    "    truth_answer = truth.answer\n",
    "    proposed_answer = pred.answer\n",
    "    similarity_question = f\"\"\"Does the assessed text have the same meaning as the gold_standard text provided?\n",
    "Gold Standard: \"{truth_answer}\"\n",
    "Assessed Answer: \"{proposed_answer}\"\n",
    "Provide only a yes or no answer.\"\"\"\n",
    "    \n",
    "    with dspy.context(lm=ollama_model):\n",
    "        assessor = dspy.Predict(AssessBasedOnQuestion)\n",
    "        raw_similarity_result = assessor(assessed_text=proposed_answer, assessment_question=similarity_question)\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Truth Answer: {truth_answer}\")\n",
    "    print(f\"Proposed Answer: {proposed_answer}\")\n",
    "    print(f\"Assessment Answer: {raw_similarity_result.assessment_answer}\")\n",
    "\n",
    "    raw_similarity = raw_similarity_result.assessment_answer.lower().strip()\n",
    "    same_meaning = raw_similarity == 'yes'\n",
    "    return same_meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ari_metric(truth, pred, trace=None):\n",
    "    truth_answer =  truth.answer\n",
    "    proposed_answer = pred.answer\n",
    "    \n",
    "    gold_ari = automated_readability_index(truth_answer)\n",
    "    pred_ari = automated_readability_index(proposed_answer)\n",
    "    print(f\"ARI {gold_ari} => {pred_ari}\")\n",
    "    ari_result = pred_ari <= 7.01\n",
    "    return ari_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_metric(provided_example, predicted, trace=None):\n",
    "    similarity = similarity_metric(provided_example, predicted, trace)\n",
    "    ari = ari_metric(provided_example, predicted, trace)\n",
    "    if similarity and ari:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What characteristics should data products have according to Zhamak Dehgahi?\n",
      "Truth Answer: Data products should be discoverable, addressable, trustworthy, self-describing, interoperable, and secure.\n",
      "Proposed Answer: According to Zhamak Dehgahi, a thought leader in the field of Data Mesh architecture, effective data products should possess the following characteristics: 1) consumer-driven, 2) domain-specific, 3) composable, 4) owned by a team or individual, 5) versioned, and 6) secure. Additionally, they should be scalable to handle increasing amounts of data and growing numbers of consumers.\n",
      "Assessment Answer: No. The assessed text mentions different characteristics for effective data products than the gold standard text provides.\n",
      "ARI 25.6 => 20.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Why is it considered a lost opportunity that most data products only support one or two use cases?\n",
      "Truth Answer: It's a lost opportunity because it reflects underutilization and limits the potential benefits across various teams, especially in organizations with decentralized structures or those implementing data mesh.\n",
      "Proposed Answer: Supporting multiple use cases for data products is beneficial because it increases value, efficiency and consistency, flexibility, improves collaboration, saves costs, and enables better decision-making. Limiting these products to just one or two use cases can be considered a lost opportunity as they may not fully leverage the potential of the data being collected and processed within an organization.\n",
      "Assessment Answer: Yes. Both texts convey that supporting multiple use cases for data products is beneficial, and limiting them to just one or two can result in missed opportunities for maximizing their value within an organization.\n",
      "ARI 23.64 => 20.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the benefit of creating a central marketplace or catalog for internal data products?\n",
      "Truth Answer: Creating a central marketplace or catalog helps raise awareness and can convince skeptical data consumers to start using the data products.\n",
      "Proposed Answer: Creating a central marketplace or catalog for internal data products offers several benefits that can lead to faster growth and efficiency within an organization. These benefits include improved discoverability of available data products, faster data access with consistent security policies and compliance, increased collaboration between teams, and standardization across the organization's handling of internal data.\n",
      "Assessment Answer: Yes. While there are some differences in emphasis and language used, both texts convey the idea that creating a central marketplace or catalog for internal data products can bring about benefits such as improved discoverability, faster access with consistent security policies, increased collaboration, and standardization within an organization.\n",
      "ARI 15.76 => 23.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How can trustworthiness in data products be achieved?\n",
      "Truth Answer: Trustworthiness in data products can be achieved by being transparent about information quality metrics and performance promises.\n",
      "Proposed Answer: To achieve trustworthiness in data products, follow these steps:\n",
      "1. Collect and process high-quality data ethically and representatively.\n",
      "2. Ensure security throughout all stages of processing by securing access to raw data and implementing encryption and other protective measures for sensitive information.\n",
      "3. Provide transparency into how the data was collected, processed, and analyzed through documentation or tools that allow users to explore underlying data themselves.\n",
      "4. Implement robust validation checks throughout all stages of processing to ensure accuracy and consistency in your data product.\n",
      "5. Continuously monitor performance and quality of your data products for early identification of issues and prevention of potential misuse or misunderstanding by users.\n",
      "6. Ensure compliance with relevant regulations, such as GDPR or HIPAA, to maintain trustworthiness in handling sensitive information.\n",
      "Assessment Answer: Yes. The assessed text and gold standard both discuss the importance of transparency and security in achieving trustworthiness in data products, although they may focus on different aspects of these concepts.\n",
      "ARI 18.38 => 11.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How does Thoughtworks identify potential data products?\n",
      "Truth Answer: Thoughtworks identifies potential data products by working backwards from the use case using the Jobs to be Done (JTBD) framework.\n",
      "Proposed Answer: Thoughtworks identifies potential data products through a methodology called Data Mesh, which involves several steps including identifying business capabilities, discovering relevant data sources, validating the data for accuracy and completeness, building pipelines to move the data, and developing data products based on the requirements of the use case.\n",
      "Assessment Answer: Yes. Both texts describe methods used by Thoughtworks to identify potential data products. The first text mentions Data Mesh as the methodology, while the gold standard text refers to working backwards from the use case using Jobs to be Done (JTBD). These approaches are related; JTBD is one way of identifying business capabilities and use cases that can guide data product development within a Data Mesh framework.\n",
      "ARI 14.71 => 32.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How can the deployment of data monitors be automated?\n",
      "Truth Answer: The deployment of data monitors can be automated using the Monte Carlo CLI as part of the CI/CD pipeline.\n",
      "Proposed Answer: To automate the deployment of data monitors, you can follow these steps:\n",
      "\n",
      "1. Create a template or configuration file for setting up new data monitor instances using Infrastructure as Code (IaC) tools like Terraform, CloudFormation, or Ansible.\n",
      "2. Write scripts to install the required monitoring software on target machines automatically using package managers like apt-get or yum for Linux systems and PowerShell for Windows systems.\n",
      "3. Set up a Continuous Integration (CI) pipeline that can build and deploy these configurations as part of an application release process, using tools such as Jenkins, GitLab CI/CD, or Travis CI.\n",
      "4. Implement monitoring for the monitoring infrastructure itself to detect any issues with the deployment and configuration of data monitors using tools like Nagios, Zabbix, or Prometheus.\n",
      "5. Use containerization technologies such as Docker or Kubernetes to manage the deployment of data monitors at scale.\n",
      "6. Implement automated testing for your monitoring scripts and configurations using tools like Selenium or TestComplete to ensure that new changes do not introduce any regressions in the functionality or performance of your data monitors. This will help maintain a stable and reliable monitoring infrastructure over time.\n",
      "Assessment Answer: No, the assessed text goes beyond just using Monte Carlo CLI for automating the deployment of data monitors as stated in the gold standard text. It provides additional steps like creating templates or configuration files, writing scripts to install monitoring software, setting up a CI pipeline, implementing monitoring for infrastructure itself, using containerization technologies and automated testing.\n",
      "ARI 9.64 => 9.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [03:09<00:00, 27.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the considerations to design the right data product?\n",
      "Truth Answer: Key considerations in designing the right data products are its fulfillment to the use case for a given domain, along with compliance to slo and slis, support for output ports based on persona, metadata for discoverability and access and quality aspects to deliver trust.\n",
      "Proposed Answer: To design the right data product, consider the following steps and key considerations at each stage:\n",
      "\n",
      "1. Understanding your audience and their needs: Identify who will be using your data product, their goals, pain points, motivations, behaviors, and desired outcomes. This includes understanding their use cases and how they plan to utilize the data product.\n",
      "2. Defining the problem statement: Clearly define the specific business or organizational problem that your data product aims to solve by being as specific as possible about the desired outcomes and benefits of solving this issue with data.\n",
      "3. Collecting relevant data: Identify which types of structured (databases) and unstructured (text files, images) data are needed to address the defined problem, where it can be sourced from, and how it will be collected and stored securely using appropriate methods like APIs or web scraping.\n",
      "4. Processing raw data: Preprocess your raw data by cleaning, transforming, aggregating, enriching it as needed using tools such as ETL processes or machine learning algorithms to extract insights from the data.\n",
      "5. Analyzing and interpreting processed data: Apply appropriate analytical techniques (statistical analysis or predictive modeling) to gain insights from your preprocessed data and interpret these findings in a meaningful way for your audience, considering their context and needs.\n",
      "6. Visualizing and communicating results: Present the results of your analysis in an easily understandable format that caters to the needs of your intended audience (charts, graphs, or tables). Ensure clear context around what each visualization represents and how it relates to the defined problem statement.\n",
      "Assessment Answer: Yes. The assessed text covers key considerations for designing data products that align with use cases, fulfill specific business problems, collect relevant data, process raw data, analyze processed data, visualize and communicate results while the gold standard text discusses aspects such as fulfillment to a given domain, compliance to SLO and SLIs, support for output ports based on persona, metadata for discoverability and accessibility and quality aspects. Both texts are discussing the design of effective data products with similar key considerations but using slightly different terminologies.\n",
      "ARI 24.98 => 11.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "config = dict(max_bootstrapped_demos=4, max_labeled_demos=4)\n",
    "optimizer = BootstrapFewShot(metric=overall_metric, **config)\n",
    "optimizer.max_errors = 1 # helpful to debug errors faster\n",
    "optimized_cot = optimizer.compile(RAG(), trainset=trainset, valset=devset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]Question: What characteristics should data products have according to Zhamak Dehgahi?\n",
      "Truth Answer: Data products should be discoverable, addressable, trustworthy, self-describing, interoperable, and secure.\n",
      "Proposed Answer: According to Zhamak Dehgahi, a thought leader in Data Mesh architecture, data products should have the following characteristics:\n",
      "\n",
      "1. Domain-driven: A data product is designed and developed with the specific needs of a particular business domain in mind. It provides value to that domain and aligns with its goals and objectives.\n",
      "2. Self-contained: Each data product should be self-contained, providing all necessary functionality for consuming its data without relying on other products or external systems.\n",
      "3. Consumable: Data products should be easy to consume by their intended audience through APIs, user interfaces, or other means of access. They should also provide clear documentation and support for usage.\n",
      "4. Versioned: As data evolves over time, data products should provide versioning capabilities so that consumers can continue using previous versions while new ones are being adopted.\n",
      "5. Testable: Data products should be designed with testing in mind, including unit tests, integration tests, and end-to-end tests to ensure reliability and maintainability over time.\n",
      "6. Scalable: As the amount of data grows, data products should be able to scale horizontally or vertically without impacting performance or availability for consumers.\n",
      "Assessment Answer: No, the assessed text and gold standard have different meanings. The assessed text focuses on characteristics of data products in Data Mesh architecture as described by Zhamak Dehgahi, while the gold standard lists six general properties for data products without specifying a particular framework or thought leader.\n",
      "Average Metric: 0 / 1  (0.0):  14%|█▍        | 1/7 [00:31<03:08, 31.36s/it]Question: Why is it considered a lost opportunity that most data products only support one or two use cases?\n",
      "Truth Answer: It's a lost opportunity because it reflects underutilization and limits the potential benefits across various teams, especially in organizations with decentralized structures or those implementing data mesh.\n",
      "Proposed Answer: The statement \"most data products only support one or two use cases\" can be considered a lost opportunity because it may limit the potential insights that organizations can gain from their data, lead to inefficiencies in development and maintenance, provide less competitive advantage, result in suboptimal user experiences, and hinder scalability. By designing data products that cater to various applications or use cases, organizations can unlock hidden opportunities for innovation and growth while streamlining operations and enhancing the overall user experience.\n",
      "Assessment Answer: Yes. Both texts convey the idea of missed opportunities when data products support only one or two use cases instead of catering to various applications or use cases within an organization.\n",
      "Average Metric: 0 / 2  (0.0):  29%|██▊       | 2/7 [00:54<02:13, 26.71s/it]Question: What is the benefit of creating a central marketplace or catalog for internal data products?\n",
      "Truth Answer: Creating a central marketplace or catalog helps raise awareness and can convince skeptical data consumers to start using the data products.\n",
      "Proposed Answer: Creating a central marketplace or catalog for internal data products offers several benefits for an organization. Firstly, it improves discoverability by providing a single platform where teams can easily find relevant data products, saving time and effort spent on searching across multiple sources. Secondly, it ensures proper access control mechanisms are in place through its accessibility feature, protecting sensitive information while granting authorized teams easy access to the necessary data. Thirdly, it encourages reuse and sharing of data products across different teams leading to increased efficiency by reducing redundancy and potential errors. Fourthly, centralized management enables versioning and maintenance of data products ensuring everyone uses the latest accurate information with ease for rollbacks if needed. Fifthly, it enforces organizational policies around data usage, security, privacy etc., maintaining consistent standards across teams. Lastly, it fosters collaboration and innovation by providing a platform where teams can discover each other's work and potentially build upon them.\n",
      "Assessment Answer: Yes. The assessed text and the gold standard text convey similar meanings despite some differences in emphasis and detail. Both texts discuss the benefits of creating a central marketplace or catalog for internal data products, including improved discoverability, access control, reuse and sharing, versioning and maintenance, enforcement of organizational policies, and fostering collaboration and innovation.\n",
      "Average Metric: 0 / 3  (0.0):  43%|████▎     | 3/7 [01:24<01:51, 27.87s/it]Question: How can trustworthiness in data products be achieved?\n",
      "Truth Answer: Trustworthiness in data products can be achieved by being transparent about information quality metrics and performance promises.\n",
      "Proposed Answer: To achieve trustworthiness in data products, there are several steps that can be taken:\n",
      "\n",
      "1. Ethical and Transparent Data Collection: Collect data ethically with informed consent from all relevant parties. Have clear policies about what data is being collected, how it will be used, and who has access to it. Implement robust security measures to protect against unauthorized access or breaches of sensitive information.\n",
      "2. Explainable Data Processing Algorithms: Ensure that the algorithms used for processing data are transparent and explainable so users understand how their data is being analyzed and what insights are being derived from it. Provide clear documentation about underlying models, and make available tools for users to interact with results in a safe manner.\n",
      "3. Accurate Data Quality: Ensure that the data used in your products is accurate, complete, timely, and relevant to its intended use case. Implement robust data validation checks at every stage of processing and have processes for identifying and addressing any errors or inconsistencies that may arise over time.\n",
      "4. Strong Data Security: Implement strong security measures to protect the confidentiality, integrity, and availability of your data products. Encrypt sensitive information both in transit and at rest, implement access controls to ensure only authorized users have access to specific data sets, and perform regular vulnerability assessments and penetration testing to identify potential weaknesses or threats.\n",
      "5. Clear Data Governance: Establish clear policies for managing the lifecycle of your data products including storage, access, sharing, and retirement over time. Have processes in place for addressing any ethical concerns that may arise from the use of your data products.\n",
      "Assessment Answer: Yes. The assessed text and gold standard both discuss the importance of trustworthiness in data products, with an emphasis on transparency, accuracy, security, and governance to achieve this goal. While there may be some differences in language or specific details between the two texts, they convey similar overall messages about best practices for creating reliable and ethical data products.\n",
      "Average Metric: 0 / 4  (0.0):  57%|█████▋    | 4/7 [01:58<01:31, 30.56s/it]Question: How does Thoughtworks identify potential data products?\n",
      "Truth Answer: Thoughtworks identifies potential data products by working backwards from the use case using the Jobs to be Done (JTBD) framework.\n",
      "Proposed Answer: Thoughtworks identifies potential data products through various means, including partnerships and collaborations with organizations, working closely with clients in different industries, engaging in research and thought leadership activities, investing in startups or acquiring companies, developing internal projects using their expertise, and collaborating with other industry experts. They may also engage in open innovation initiatives to invite external ideas for new solutions using data as a key component.\n",
      "Assessment Answer: No. The assessed text mentions several methods used by Thoughtworks to identify potential data products, while the gold standard text specifies that they use the Jobs to be Done (JTBD) framework to work backwards from the use case. These are not exactly the same thing.\n",
      "Average Metric: 0 / 5  (0.0):  71%|███████▏  | 5/7 [02:21<00:55, 27.71s/it]Question: How can the deployment of data monitors be automated?\n",
      "Truth Answer: The deployment of data monitors can be automated using the Monte Carlo CLI as part of the CI/CD pipeline.\n",
      "Proposed Answer: To automate the deployment of data monitors, follow these steps:\n",
      "1. Create a template or configuration file for setting up new data monitor instances using infrastructure as code tools like Terraform, CloudFormation, or Ansible playbooks.\n",
      "2. Write scripts to automatically generate required configuration files based on input parameters such as monitoring targets and thresholds in languages like Python, Bash, or PowerShell.\n",
      "3. Set up a CI/CD pipeline using Jenkins, GitLab CI/CD, or CircleCI to build the configuration files and deploy them when changes are made.\n",
      "4. Use containerization technologies such as Docker or Kubernetes for packaging and deployment of data monitors as containers for consistent configurations across environments.\n",
      "5. Implement automated testing before deployment using tools like Selenium, TestComplete, or Appium to verify monitor functionality and accuracy.\n",
      "6. Set up alerting mechanisms with monitoring tools (Nagios, Zabbix, Prometheus) and communication channels (email, SMS, Slack).\n",
      "7. Use version control systems like Git or SVN for managing configuration files and scripts.\n",
      "Assessment Answer: Yes. The assessed text describes different methods to automate the deployment of data monitors compared to the gold standard text, but they both refer to automating the deployment process.\n",
      "Average Metric: 0 / 6  (0.0):  86%|████████▌ | 6/7 [02:50<00:28, 28.13s/it]Question: What are the considerations to design the right data product?\n",
      "Truth Answer: Key considerations in designing the right data products are its fulfillment to the use case for a given domain, along with compliance to slo and slis, support for output ports based on persona, metadata for discoverability and access and quality aspects to deliver trust.\n",
      "Proposed Answer: To design the right data product, consider the following steps and factors:\n",
      "\n",
      "1. Identify Business Objectives: Understand the problem you're trying to solve and what questions need answering from your data. Determine how this data will help drive better decision-making or improve processes within your organization.\n",
      "2. Define Data Scope: Determine which specific data elements are required for addressing the business objectives identified in step 1, including both internal and external sources.\n",
      "3. Choose Appropriate Data Modeling Techniques: Depending on the nature of your data, select modeling techniques such as relational databases, NoSQL databases, or graph databases to effectively store and manage your data product.\n",
      "4. Design User Interface (UI): Create a user-friendly interface that allows users to easily access and interact with the data in meaningful ways through dashboards, reports, or other visualizations.\n",
      "5. Ensure Data Security: Implement security measures like encryption, access controls, and authentication protocols to protect sensitive information from unauthorized users.\n",
      "6. Process Data in Real Time (if necessary): For certain applications such as fraud detection or predictive analytics, process data in real time or near real-time using technologies like Apache Kafka, Apache Flink, and Apache Storm.\n",
      "7. Ensure Scalability: Design your data product to handle increasing amounts of data and users without compromising performance. This may involve implementing sharding techniques, load balancing strategies, or other distributed computing solutions.\n",
      "8. Consider Data Governance: Establish policies for managing the availability, usability, integrity, and security of your data product over its entire lifecycle.\n",
      "9. Implement Monitoring and Alerts\n",
      "Assessment Answer: Yes, both texts discuss the steps involved in designing an effective data product for an organization. However, they may not use identical language to describe each step, but the overall meaning is similar.\n",
      "Average Metric: 0 / 7  (0.0): 100%|██████████| 7/7 [03:24<00:00, 29.17s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_80c00 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_80c00 td {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_80c00_row0_col0, #T_80c00_row0_col1, #T_80c00_row0_col2, #T_80c00_row0_col3, #T_80c00_row0_col4, #T_80c00_row1_col0, #T_80c00_row1_col1, #T_80c00_row1_col2, #T_80c00_row1_col3, #T_80c00_row1_col4, #T_80c00_row2_col0, #T_80c00_row2_col1, #T_80c00_row2_col2, #T_80c00_row2_col3, #T_80c00_row2_col4, #T_80c00_row3_col0, #T_80c00_row3_col1, #T_80c00_row3_col2, #T_80c00_row3_col3, #T_80c00_row3_col4, #T_80c00_row4_col0, #T_80c00_row4_col1, #T_80c00_row4_col2, #T_80c00_row4_col3, #T_80c00_row4_col4 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "  max-width: 400px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_80c00\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_80c00_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_80c00_level0_col1\" class=\"col_heading level0 col1\" >example_answer</th>\n",
       "      <th id=\"T_80c00_level0_col2\" class=\"col_heading level0 col2\" >context</th>\n",
       "      <th id=\"T_80c00_level0_col3\" class=\"col_heading level0 col3\" >pred_answer</th>\n",
       "      <th id=\"T_80c00_level0_col4\" class=\"col_heading level0 col4\" >similarity_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_80c00_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_80c00_row0_col0\" class=\"data row0 col0\" >What characteristics should data products have according to Zhamak Dehgahi?</td>\n",
       "      <td id=\"T_80c00_row0_col1\" class=\"data row0 col1\" >Data products should be discoverable, addressable, trustworthy, self-describing, interoperable, and secure.</td>\n",
       "      <td id=\"T_80c00_row0_col2\" class=\"data row0 col2\" >['[ ![Thoughtworks](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/thoughtworks-logo.svg) ](/en-in \"Thoughtworks\")\\n\\nMenu\\n\\nClose\\n\\n * [What we do ](/en-in/what-we-do \"What we do\")\\n\\n * [ Go to overview ](/en-in/what-we-do)\\n * ### Services\\n\\n * [ Artificial Intelligence...</td>\n",
       "      <td id=\"T_80c00_row0_col3\" class=\"data row0 col3\" >According to Zhamak Dehgahi, a thought leader in Data Mesh architecture, data products should have the following characteristics: 1. Domain-driven: A data product is designed...</td>\n",
       "      <td id=\"T_80c00_row0_col4\" class=\"data row0 col4\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_80c00_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_80c00_row1_col0\" class=\"data row1 col0\" >Why is it considered a lost opportunity that most data products only support one or two use cases?</td>\n",
       "      <td id=\"T_80c00_row1_col1\" class=\"data row1 col1\" >It's a lost opportunity because it reflects underutilization and limits the potential benefits across various teams, especially in organizations with decentralized structures or those implementing...</td>\n",
       "      <td id=\"T_80c00_row1_col2\" class=\"data row1 col2\" >['[ ![Thoughtworks](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/thoughtworks-logo.svg) ](/en-in \"Thoughtworks\")\\n\\nMenu\\n\\nClose\\n\\n * [What we do ](/en-in/what-we-do \"What we do\")\\n\\n * [ Go to overview ](/en-in/what-we-do)\\n * ### Services\\n\\n * [ Artificial Intelligence...</td>\n",
       "      <td id=\"T_80c00_row1_col3\" class=\"data row1 col3\" >The statement \"most data products only support one or two use cases\" can be considered a lost opportunity because it may limit the potential insights...</td>\n",
       "      <td id=\"T_80c00_row1_col4\" class=\"data row1 col4\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_80c00_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_80c00_row2_col0\" class=\"data row2 col0\" >What is the benefit of creating a central marketplace or catalog for internal data products?</td>\n",
       "      <td id=\"T_80c00_row2_col1\" class=\"data row2 col1\" >Creating a central marketplace or catalog helps raise awareness and can convince skeptical data consumers to start using the data products.</td>\n",
       "      <td id=\"T_80c00_row2_col2\" class=\"data row2 col2\" >['[ ![Thoughtworks](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/thoughtworks-logo.svg) ](/en-in \"Thoughtworks\")\\n\\nMenu\\n\\nClose\\n\\n * [What we do ](/en-in/what-we-do \"What we do\")\\n\\n * [ Go to overview ](/en-in/what-we-do)\\n * ### Services\\n\\n * [ Artificial Intelligence...</td>\n",
       "      <td id=\"T_80c00_row2_col3\" class=\"data row2 col3\" >Creating a central marketplace or catalog for internal data products offers several benefits for an organization. Firstly, it improves discoverability by providing a single platform...</td>\n",
       "      <td id=\"T_80c00_row2_col4\" class=\"data row2 col4\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_80c00_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_80c00_row3_col0\" class=\"data row3 col0\" >How can trustworthiness in data products be achieved?</td>\n",
       "      <td id=\"T_80c00_row3_col1\" class=\"data row3 col1\" >Trustworthiness in data products can be achieved by being transparent about information quality metrics and performance promises.</td>\n",
       "      <td id=\"T_80c00_row3_col2\" class=\"data row3 col2\" >['[ ![Thoughtworks](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/thoughtworks-logo.svg) ](/en-in \"Thoughtworks\")\\n\\nMenu\\n\\nClose\\n\\n * [What we do ](/en-in/what-we-do \"What we do\")\\n\\n * [ Go to overview ](/en-in/what-we-do)\\n * ### Services\\n\\n * [ Artificial Intelligence...</td>\n",
       "      <td id=\"T_80c00_row3_col3\" class=\"data row3 col3\" >To achieve trustworthiness in data products, there are several steps that can be taken: 1. Ethical and Transparent Data Collection: Collect data ethically with informed...</td>\n",
       "      <td id=\"T_80c00_row3_col4\" class=\"data row3 col4\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_80c00_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_80c00_row4_col0\" class=\"data row4 col0\" >How does Thoughtworks identify potential data products?</td>\n",
       "      <td id=\"T_80c00_row4_col1\" class=\"data row4 col1\" >Thoughtworks identifies potential data products by working backwards from the use case using the Jobs to be Done (JTBD) framework.</td>\n",
       "      <td id=\"T_80c00_row4_col2\" class=\"data row4 col2\" >['[ ![Thoughtworks](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/thoughtworks-logo.svg) ](/en-in \"Thoughtworks\")\\n\\nMenu\\n\\nClose\\n\\n * [What we do ](/en-in/what-we-do \"What we do\")\\n\\n * [ Go to overview ](/en-in/what-we-do)\\n * ### Services\\n\\n * [ Artificial Intelligence...</td>\n",
       "      <td id=\"T_80c00_row4_col3\" class=\"data row4 col3\" >Thoughtworks identifies potential data products through various means, including partnerships and collaborations with organizations, working closely with clients in different industries, engaging in research and...</td>\n",
       "      <td id=\"T_80c00_row4_col4\" class=\"data row4 col4\" >False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x356215550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div style='\n",
       "                    text-align: center;\n",
       "                    font-size: 16px;\n",
       "                    font-weight: bold;\n",
       "                    color: #555;\n",
       "                    margin: 10px 0;'>\n",
       "                    ... 2 more rows not displayed ...\n",
       "                </div>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]ARI 25.6 => 17.11\n",
      "Average Metric: 0 / 1  (0.0):  14%|█▍        | 1/7 [00:22<02:15, 22.63s/it]ARI 23.64 => 11.66\n",
      "Average Metric: 0 / 2  (0.0):  29%|██▊       | 2/7 [00:51<02:10, 26.16s/it]ARI 15.76 => 12.96\n",
      "Average Metric: 0 / 3  (0.0):  43%|████▎     | 3/7 [01:16<01:42, 25.66s/it]ARI 18.38 => 30.49\n",
      "Average Metric: 0 / 4  (0.0):  57%|█████▋    | 4/7 [01:37<01:11, 23.94s/it]ARI 14.71 => 20.04\n",
      "Average Metric: 0 / 5  (0.0):  71%|███████▏  | 5/7 [01:54<00:42, 21.46s/it]ARI 9.64 => 9.79\n",
      "Average Metric: 0 / 6  (0.0):  86%|████████▌ | 6/7 [02:19<00:22, 22.65s/it]ARI 24.98 => 12.23\n",
      "Average Metric: 0 / 7  (0.0): 100%|██████████| 7/7 [02:48<00:00, 24.03s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d8668 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_d8668 td {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_d8668_row0_col0, #T_d8668_row0_col1, #T_d8668_row0_col2, #T_d8668_row0_col3, #T_d8668_row0_col4, #T_d8668_row1_col0, #T_d8668_row1_col1, #T_d8668_row1_col2, #T_d8668_row1_col3, #T_d8668_row1_col4, #T_d8668_row2_col0, #T_d8668_row2_col1, #T_d8668_row2_col2, #T_d8668_row2_col3, #T_d8668_row2_col4, #T_d8668_row3_col0, #T_d8668_row3_col1, #T_d8668_row3_col2, #T_d8668_row3_col3, #T_d8668_row3_col4, #T_d8668_row4_col0, #T_d8668_row4_col1, #T_d8668_row4_col2, #T_d8668_row4_col3, #T_d8668_row4_col4 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "  max-width: 400px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d8668\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d8668_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_d8668_level0_col1\" class=\"col_heading level0 col1\" >example_answer</th>\n",
       "      <th id=\"T_d8668_level0_col2\" class=\"col_heading level0 col2\" >context</th>\n",
       "      <th id=\"T_d8668_level0_col3\" class=\"col_heading level0 col3\" >pred_answer</th>\n",
       "      <th id=\"T_d8668_level0_col4\" class=\"col_heading level0 col4\" >ari_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d8668_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_d8668_row0_col0\" class=\"data row0 col0\" >What characteristics should data products have according to Zhamak Dehgahi?</td>\n",
       "      <td id=\"T_d8668_row0_col1\" class=\"data row0 col1\" >Data products should be discoverable, addressable, trustworthy, self-describing, interoperable, and secure.</td>\n",
       "      <td id=\"T_d8668_row0_col2\" class=\"data row0 col2\" >['[ ![Thoughtworks](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/thoughtworks-logo.svg) ](/en-in \"Thoughtworks\")\\n\\nMenu\\n\\nClose\\n\\n * [What we do ](/en-in/what-we-do \"What we do\")\\n\\n * [ Go to overview ](/en-in/what-we-do)\\n * ### Services\\n\\n * [ Artificial Intelligence...</td>\n",
       "      <td id=\"T_d8668_row0_col3\" class=\"data row0 col3\" >According to Zhamak Dehgahi's perspective on data products in the context of Data Mesh architecture, effective data products should possess the following characteristics: 1) Consumer-driven:...</td>\n",
       "      <td id=\"T_d8668_row0_col4\" class=\"data row0 col4\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8668_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_d8668_row1_col0\" class=\"data row1 col0\" >Why is it considered a lost opportunity that most data products only support one or two use cases?</td>\n",
       "      <td id=\"T_d8668_row1_col1\" class=\"data row1 col1\" >It's a lost opportunity because it reflects underutilization and limits the potential benefits across various teams, especially in organizations with decentralized structures or those implementing...</td>\n",
       "      <td id=\"T_d8668_row1_col2\" class=\"data row1 col2\" >['[ ![Thoughtworks](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/thoughtworks-logo.svg) ](/en-in \"Thoughtworks\")\\n\\nMenu\\n\\nClose\\n\\n * [What we do ](/en-in/what-we-do \"What we do\")\\n\\n * [ Go to overview ](/en-in/what-we-do)\\n * ### Services\\n\\n * [ Artificial Intelligence...</td>\n",
       "      <td id=\"T_d8668_row1_col3\" class=\"data row1 col3\" >The statement \"most data products only support one or two use cases\" implies that there are potential uses for the data beyond what is currently...</td>\n",
       "      <td id=\"T_d8668_row1_col4\" class=\"data row1 col4\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8668_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_d8668_row2_col0\" class=\"data row2 col0\" >What is the benefit of creating a central marketplace or catalog for internal data products?</td>\n",
       "      <td id=\"T_d8668_row2_col1\" class=\"data row2 col1\" >Creating a central marketplace or catalog helps raise awareness and can convince skeptical data consumers to start using the data products.</td>\n",
       "      <td id=\"T_d8668_row2_col2\" class=\"data row2 col2\" >['[ ![Thoughtworks](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/thoughtworks-logo.svg) ](/en-in \"Thoughtworks\")\\n\\nMenu\\n\\nClose\\n\\n * [What we do ](/en-in/what-we-do \"What we do\")\\n\\n * [ Go to overview ](/en-in/what-we-do)\\n * ### Services\\n\\n * [ Artificial Intelligence...</td>\n",
       "      <td id=\"T_d8668_row2_col3\" class=\"data row2 col3\" >Creating a central marketplace or catalog for internal data products can bring several advantages to an organization. Some of these benefits include: 1. Discoverability: A...</td>\n",
       "      <td id=\"T_d8668_row2_col4\" class=\"data row2 col4\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8668_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_d8668_row3_col0\" class=\"data row3 col0\" >How can trustworthiness in data products be achieved?</td>\n",
       "      <td id=\"T_d8668_row3_col1\" class=\"data row3 col1\" >Trustworthiness in data products can be achieved by being transparent about information quality metrics and performance promises.</td>\n",
       "      <td id=\"T_d8668_row3_col2\" class=\"data row3 col2\" >['[ ![Thoughtworks](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/thoughtworks-logo.svg) ](/en-in \"Thoughtworks\")\\n\\nMenu\\n\\nClose\\n\\n * [What we do ](/en-in/what-we-do \"What we do\")\\n\\n * [ Go to overview ](/en-in/what-we-do)\\n * ### Services\\n\\n * [ Artificial Intelligence...</td>\n",
       "      <td id=\"T_d8668_row3_col3\" class=\"data row3 col3\" >To achieve trustworthiness in data products, follow these steps: 1) Ensure ethical and transparent data collection with informed consent if necessary; 2) Implement robust security...</td>\n",
       "      <td id=\"T_d8668_row3_col4\" class=\"data row3 col4\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8668_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_d8668_row4_col0\" class=\"data row4 col0\" >How does Thoughtworks identify potential data products?</td>\n",
       "      <td id=\"T_d8668_row4_col1\" class=\"data row4 col1\" >Thoughtworks identifies potential data products by working backwards from the use case using the Jobs to be Done (JTBD) framework.</td>\n",
       "      <td id=\"T_d8668_row4_col2\" class=\"data row4 col2\" >['[ ![Thoughtworks](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/thoughtworks-logo.svg) ](/en-in \"Thoughtworks\")\\n\\nMenu\\n\\nClose\\n\\n * [What we do ](/en-in/what-we-do \"What we do\")\\n\\n * [ Go to overview ](/en-in/what-we-do)\\n * ### Services\\n\\n * [ Artificial Intelligence...</td>\n",
       "      <td id=\"T_d8668_row4_col3\" class=\"data row4 col3\" >Based on the information provided in the text, Thoughtworks identifies potential data products through client needs assessment, market trends analysis, or by leveraging their expertise...</td>\n",
       "      <td id=\"T_d8668_row4_col4\" class=\"data row4 col4\" >False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x356215550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div style='\n",
       "                    text-align: center;\n",
       "                    font-size: 16px;\n",
       "                    font-weight: bold;\n",
       "                    color: #555;\n",
       "                    margin: 10px 0;'>\n",
       "                    ... 2 more rows not displayed ...\n",
       "                </div>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dspy.evaluate import Evaluate\n",
    "individual_metrics = [similarity_metric, ari_metric]\n",
    "for metric in individual_metrics:\n",
    "    evaluate = Evaluate(metric=metric, devset=trainset, num_threads=1, display_progress=True, display_table=5)\n",
    "    evaluate(optimized_cot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = optimized_cot.forward(\"What characteristics should data products have according to Zhamak Dehgahi?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to Zhamak Dehgahi, a good data product should have the following characteristics: 1) Event-driven, 2) Streaming, 3) Decentralized, 4) Immutable, and 5) Real-time analytics. These characteristics help make data products more responsive, flexible to changing business requirements, scalable and resilient to failures, ensure data integrity and consistency across the system, as well as enable organizations to make informed decisions quickly based on new information.'"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_cot.save(path=\"/Users/samvardhan/Desktop/DataEngineer/DSPy_Experiment/model.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Hop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "\n",
    "\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateSearchQuery(dspy.Signature):\n",
    "    \"\"\"Write a simple search query that will help answer a complex question.\"\"\"\n",
    "\n",
    "\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    query = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsp.utils import deduplicate\n",
    "\n",
    "class SimplifiedBaleen(dspy.Module):\n",
    "    def __init__(self, passages_per_hop=3, max_hops=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _ in range(max_hops)]\n",
    "        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "        self.max_hops = max_hops\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = []\n",
    "        \n",
    "        for hop in range(self.max_hops):\n",
    "            query = self.generate_query[hop](context=context, question=question).query\n",
    "            passages = self.retrieve(query).passages\n",
    "            context = deduplicate(context + passages)\n",
    "\n",
    "        pred = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=pred.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is Data Mesh?\n",
      "Predicted Answer: Data Mesh is a term used in the context of data management and processing. It refers to a decentralized architecture where data is stored and processed across multiple nodes or devices, rather than being concentrated on a single centralized location. This approach allows for more efficient use of resources, improved scalability, and enhanced security by distributing the load across multiple nodes.\n",
      "\n",
      "In this context, \"Data Mesh\" can be thought of as a network of interconnected data sources, where each node or device contributes to the overall dataset. The mesh structure enables flexible and efficient communication between these nodes, allowing for real-time updates and processing of data without relying on a single centralized point of control.\n",
      "\n",
      "The term \"Data Mesh\" is often used in contrast to traditional centralized data architectures, where all data is stored and processed on a single server or cluster of servers. In this model, the entire dataset is located on a single physical location, making it more vulnerable to failures or attacks that could impact the entire system.\n",
      "\n",
      "By distributing data across multiple nodes in a mesh architecture, Data Mesh offers several advantages over traditional centralized systems:\n",
      "\n",
      "1. Scalability: With Data Mesh, adding more nodes to the network can easily scale up processing power and storage capacity without affecting performance. This makes it easier to handle large volumes of data or high traffic loads.\n",
      "2. Flexibility: In a decentralized architecture like Data Mesh, new nodes can be added or removed as needed without disrupting the system's overall functionality. This flexibility allows for greater adaptability in response to changing business needs or technology advancements.\n",
      "3.\n",
      "Retrieved Contexts (truncated): ['[ ![Thoughtworks](/etc.clientlibs/thoughtworks/clientlibs/clientlib-\\nsite/resources/images/thoughtworks-logo.svg) ](/en-in \"Thoughtworks\")\\n\\nMenu\\n\\nClose\\n\\n  * [What we do  ](/en-in/what-we-do \"What we d...']\n"
     ]
    }
   ],
   "source": [
    "my_question = \"What is Data Mesh?\"\n",
    "\n",
    "\n",
    "# Get the prediction. This contains `pred.context` and `pred.answer`.\n",
    "uncompiled_baleen = SimplifiedBaleen()  # uncompiled (i.e., zero-shot) program\n",
    "pred = uncompiled_baleen(my_question)\n",
    "\n",
    "\n",
    "# Print the contexts and the answer.\n",
    "print(f\"Question: {my_question}\")\n",
    "print(f\"Predicted Answer: {pred.answer}\")\n",
    "print(f\"Retrieved Contexts (truncated): {[c[:200] + '...' for c in pred.context]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
